{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generic inputs for most ML tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# This is new\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# setup interactive notebook mode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "qc_data = pd.read_csv('Kaggle_Data/quality_data_ng.csv')\n",
    "qc_data.head()\n",
    "qc_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the features against the classification\n",
    "X0 = qc_data[qc_data['Accept'] == 0]\n",
    "X1 = qc_data[qc_data['Accept'] == 1]\n",
    "plt.scatter(X0['Test 1'], X0['Test 2'], color = 'red', marker = 'o', label = 'reject')\n",
    "plt.scatter(X1['Test 1'], X1['Test 2'], color = 'blue', marker = 'x', label = 'accept')\n",
    "plt.xlabel('Test 1')\n",
    "plt.ylabel('Test 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_data['t1_sq'] = qc_data['Test 1']**2\n",
    "qc_data['t2_sq'] = qc_data['Test 2']**2\n",
    "qc_data['t1_t2'] = qc_data['Test 1']*qc_data['Test 2']\n",
    "qc_data.head()\n",
    "qc_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_data.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(qc_data.drop(columns = ['Accept']), qc_data['Accept'], test_size=0.2, stratify = qc_data['Accept'], random_state=50)\n",
    "\n",
    "# In the above split the stratify = y essentially makes sure the fractions of the classification is maintained\n",
    "X_train\n",
    "X_test\n",
    "y_train\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(fit_intercept = True, solver='saga', multi_class = 'auto', penalty = 'l1', C = 10.0, max_iter = 1000)\n",
    "# If the lbfgs throws an error, try to increase max_iter (add max_iter = 1000), also try another algorithm, scaling is also suggested\n",
    "# While using multiclass case do multi_class = 'over' or 'auto'; can also try other solvers\n",
    "# While doing regularization, use penalty = 'l2' and also C = 10.0 (need to try other values too)\n",
    "\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# The following gives the mean accuracy on the given data and labels\n",
    "model.score(X_train, y_train) \n",
    "\n",
    "# This is the coefficient Beta_1, ..., Beta_7\n",
    "model.coef_\n",
    "\n",
    "# This is the coefficient Beta_0\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = pd.DataFrame(model.predict(X_test), index = X_test.index, columns = ['pred_Accept'])\n",
    "test_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "test_output.head()\n",
    "print('Percentage of correct predictions is ')\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = test_output.merge(X_test, left_index = True, right_index = True)\n",
    "test_output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(qc_data.drop(columns = ['Accept']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_prob = X_train.copy()\n",
    "data_with_prob['Accept'] = y_train\n",
    "\n",
    "# Next we give the probability of predicting 1 (in multiclass, there will be probabilities by class)\n",
    "data_with_prob['Probability'] = model.predict_proba(data_with_prob.drop(columns = ['Accept']))[:,1]\n",
    "\n",
    "data_with_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output['Probability'] = model.predict_proba(test_output.drop(columns = ['Accept', 'pred_Accept']))[:,1]\n",
    "test_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# plot the features against the classification [Training]\n",
    "X0_right = data_with_prob[(data_with_prob['Accept'] == 0) & (data_with_prob['Probability'] < 0.5)]\n",
    "X1_right = data_with_prob[(data_with_prob['Accept'] == 1) & (data_with_prob['Probability'] >= 0.5)]\n",
    "X0_wrong = data_with_prob[(data_with_prob['Accept'] == 0) & (data_with_prob['Probability'] >= 0.5)]\n",
    "X1_wrong = data_with_prob[(data_with_prob['Accept'] == 1) & (data_with_prob['Probability'] < 0.5)]\n",
    "\n",
    "plt.scatter(X0_right['Test 1'], X0_right['Test 2'], color = 'red', marker = 'o', label = 'reject accurate')\n",
    "plt.scatter(X1_right['Test 1'], X1_right['Test 2'], color = 'blue', marker = 'x', label = 'accept accurate')\n",
    "plt.scatter(X0_wrong['Test 1'], X0_wrong['Test 2'] + 0.1, color = 'black', marker = 'o', label = 'reject inaccurate')\n",
    "plt.scatter(X1_wrong['Test 1'], X1_wrong['Test 2'] - 0.1, color = 'cyan', marker = 'x', label = 'accept inaccurate')\n",
    "plt.xlabel('Test 1')\n",
    "plt.ylabel('Test 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the features against the classification [Testing]\n",
    "X0_right = test_output[(test_output['Accept'] == 0) & (test_output['Probability'] < 0.5)]\n",
    "X1_right = test_output[(test_output['Accept'] == 1) & (test_output['Probability'] >= 0.5)]\n",
    "X0_wrong = test_output[(test_output['Accept'] == 0) & (test_output['Probability'] >= 0.5)]\n",
    "X1_wrong = test_output[(test_output['Accept'] == 1) & (test_output['Probability'] < 0.5)]\n",
    "\n",
    "plt.scatter(X0_right['Test 1'], X0_right['Test 2'], color = 'red', marker = 'o', label = 'reject accurate')\n",
    "plt.scatter(X1_right['Test 1'], X1_right['Test 2'], color = 'blue', marker = 'x', label = 'accept accurate')\n",
    "plt.scatter(X0_wrong['Test 1'], X0_wrong['Test 2'] + 0.1, color = 'black', marker = 'o', label = 'reject inaccurate')\n",
    "plt.scatter(X1_wrong['Test 1'], X1_wrong['Test 2'] - 0.1, color = 'cyan', marker = 'x', label = 'accept inaccurate')\n",
    "plt.xlabel('Test 1')\n",
    "plt.ylabel('Test 2')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
