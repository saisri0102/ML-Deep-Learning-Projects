{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Generic inputs for most ML tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# setup interactive notebook mode\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read, pre-process and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# fetch data \n",
    "symmetric = True\n",
    "if symmetric: \n",
    "    data_set = pd.read_csv('../assignments_sp23/letter-recognition.csv')\n",
    "else: \n",
    "    data_set = pd.read_csv('../assignments_sp23/letter-recognition-original.csv')\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symmetric Letter Prediction ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_set.drop(columns = ['Symmetric', 'Capital_letter']), data_set['Symmetric'], test_size=0.2, stratify = data_set['Symmetric'], random_state=50)\n",
    "    # In the above split the stratify = y essentially makes sure the fractions of the classification is maintained\n",
    "    X_train\n",
    "    X_test\n",
    "    y_train\n",
    "    y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    # model = LogisticRegression(fit_intercept = True, solver='lbfgs', multi_class = 'auto', penalty = 'none')\n",
    "    # model = LogisticRegression(fit_intercept = True, solver='liblinear', multi_class = 'auto', penalty = 'l1', C = 0.1)\n",
    "    model = LogisticRegression(fit_intercept = True, solver='liblinear', multi_class = 'auto', penalty = 'l1', C = 10)\n",
    "\n",
    "    model.fit(X_train, y_train) \n",
    "\n",
    "    # The following gives the mean accuracy on the given data and labels\n",
    "    model.score(X_train, y_train) \n",
    "\n",
    "    # This is the coefficient Beta_1, ..., Beta_7\n",
    "    model.coef_\n",
    "\n",
    "    # This is the coefficient Beta_0\n",
    "    model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    print('Percentage of correct predictions is ')\n",
    "    print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    X_train.columns\n",
    "    gb = GradientBoostingClassifier(random_state=50, min_samples_split = 12, min_samples_leaf = 6, max_depth = 4, n_estimators = 100)\n",
    "\n",
    "    gb = gb.fit(X_train, y_train) \n",
    "    gb.score(X_train, y_train) \n",
    "\n",
    "    # gb.feature_importances_\n",
    "    feat_imp = pd.Series(gb.feature_importances_, X_train.columns.values).sort_values(ascending=False)\n",
    "\n",
    "    feat_imp_table = pd.DataFrame(feat_imp)\n",
    "    feat_imp_table = feat_imp_table.reset_index()\n",
    "    feat_imp_table.columns = ['Features', 'Values']\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.figure(figsize=[40,20], dpi = 50)\n",
    "    feat_imp.head(12)\n",
    "\n",
    "    test_output = pd.DataFrame(gb.predict(X_test), index = X_test.index, columns = ['pred_Y'])\n",
    "\n",
    "    test_output.head()\n",
    "    test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "    test_output.head()\n",
    "    print('Fraction of correct classification ')\n",
    "    gb.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    X_train.columns\n",
    "    rf = RandomForestClassifier(random_state=50, min_samples_leaf = 6, max_features = \"sqrt\", n_estimators = 100)\n",
    "\n",
    "    rf = rf.fit(X_train, y_train) \n",
    "    rf.score(X_train, y_train) \n",
    "\n",
    "    # rf.feature_importances_\n",
    "    feat_imp = pd.Series(rf.feature_importances_, X_train.columns.values).sort_values(ascending=False)\n",
    "\n",
    "    feat_imp_table = pd.DataFrame(feat_imp)\n",
    "    feat_imp_table = feat_imp_table.reset_index()\n",
    "    feat_imp_table.columns = ['Features', 'Values']\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    plt.figure(figsize=[40,20], dpi = 50)\n",
    "    feat_imp.head(12)\n",
    "\n",
    "    test_output = pd.DataFrame(rf.predict(X_test), index = X_test.index, columns = ['pred_Y'])\n",
    "\n",
    "    test_output.head()\n",
    "    test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "    test_output.head()\n",
    "    print('Fraction of correct classification ')\n",
    "    rf.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    # Create regression matrices\n",
    "\n",
    "    dtrain_class = xgb.DMatrix(X_train, y_train, enable_categorical=True)\n",
    "\n",
    "    dtest_class = xgb.DMatrix(X_test, y_test, enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    # Define hyperparameters\n",
    "\n",
    "    # params = {\"objective\": \"reg:squarederror\", \"tree_method\": \"gpu_hist\"}\n",
    "    # Use above if we have GPU\n",
    "    params = {\"objective\": \"binary:hinge\", \"tree_method\": \"exact\", \"max_depth\" : 4, \"learning_rate\" : 0.4} # use \"tree_method\" : \"hist\" if you need speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    n = 100\n",
    "\n",
    "    model = xgb.train(\n",
    "\n",
    "       params=params,\n",
    "\n",
    "       dtrain=dtrain_class,\n",
    "\n",
    "       num_boost_round=n,\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    preds = model.predict(dtest_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    test_output = pd.DataFrame(preds, index = X_test.index, columns = ['pred_Y'])\n",
    "    test_output.head()\n",
    "    test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "    test_output.head()\n",
    "    sum(test_output['Symmetric'] == 0)\n",
    "    sum(test_output['Symmetric'] == 1)\n",
    "    sum(test_output['pred_Y'] == 0)\n",
    "    sum(test_output['pred_Y'] == 1)\n",
    "    sum(test_output['Symmetric'] == test_output['pred_Y'])\n",
    "    len(test_output)\n",
    "    sum(test_output['Symmetric'] == test_output['pred_Y'])/len(test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Capital Letter ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if symmetric: \n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_set.drop(columns = ['Symmetric', 'Capital_letter']), data_set['Capital_letter'], test_size=0.2, stratify = data_set['Symmetric'], random_state=50)\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_set.drop(columns = ['Capital_letter']), data_set['Capital_letter'], test_size=0.2, stratify = data_set['Capital_letter'], random_state=50)\n",
    "# In the above split the stratify = y essentially makes sure the fractions of the classification is maintained\n",
    "X_train\n",
    "X_test\n",
    "y_train\n",
    "y_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = LogisticRegression(fit_intercept = True, solver='lbfgs', multi_class = 'auto', penalty = 'none')\n",
    "model = LogisticRegression(fit_intercept = True, solver='liblinear', multi_class = 'auto', penalty = 'l1', C = 0.1)\n",
    "# model = LogisticRegression(fit_intercept = True, solver='liblinear', multi_class = 'auto', penalty = 'l1', C = 10)\n",
    "\n",
    "model.fit(X_train, y_train) \n",
    "\n",
    "# The following gives the mean accuracy on the given data and labels\n",
    "model.score(X_train, y_train) \n",
    "\n",
    "# This is the coefficient Beta_1, ..., Beta_7\n",
    "model.coef_\n",
    "\n",
    "# This is the coefficient Beta_0\n",
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of correct predictions is ')\n",
    "print(model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns\n",
    "gb = GradientBoostingClassifier(random_state=50, min_samples_split = 12, min_samples_leaf = 6, max_depth = 4, n_estimators = 100)\n",
    "\n",
    "gb = gb.fit(X_train, y_train) \n",
    "gb.score(X_train, y_train) \n",
    "\n",
    "# gb.feature_importances_\n",
    "feat_imp = pd.Series(gb.feature_importances_, X_train.columns.values).sort_values(ascending=False)\n",
    "\n",
    "feat_imp_table = pd.DataFrame(feat_imp)\n",
    "feat_imp_table = feat_imp_table.reset_index()\n",
    "feat_imp_table.columns = ['Features', 'Values']\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.figure(figsize=[40,20], dpi = 50)\n",
    "feat_imp.head(12)\n",
    "\n",
    "test_output = pd.DataFrame(gb.predict(X_test), index = X_test.index, columns = ['pred_Y'])\n",
    "\n",
    "test_output.head()\n",
    "test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "test_output.head()\n",
    "print('Fraction of correct classification ')\n",
    "gb.score(X_test, y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns\n",
    "rf = RandomForestClassifier(random_state=50, min_samples_leaf = 6, max_features = \"sqrt\", n_estimators = 100)\n",
    "\n",
    "rf = rf.fit(X_train, y_train) \n",
    "rf.score(X_train, y_train) \n",
    "\n",
    "# rf.feature_importances_\n",
    "feat_imp = pd.Series(rf.feature_importances_, X_train.columns.values).sort_values(ascending=False)\n",
    "\n",
    "feat_imp_table = pd.DataFrame(feat_imp)\n",
    "feat_imp_table = feat_imp_table.reset_index()\n",
    "feat_imp_table.columns = ['Features', 'Values']\n",
    "feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')\n",
    "plt.figure(figsize=[40,20], dpi = 50)\n",
    "feat_imp.head(12)\n",
    "\n",
    "test_output = pd.DataFrame(rf.predict(X_test), index = X_test.index, columns = ['pred_Y'])\n",
    "\n",
    "test_output.head()\n",
    "test_output = test_output.merge(y_test, left_index = True, right_index = True)\n",
    "test_output.head()\n",
    "print('Fraction of correct classification ')\n",
    "rf.score(X_test, y_test) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
