{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ef0ef47-196b-4a19-ae3e-83cf15a1fd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Donald Trump just couldn t wish all Americans ...\n",
      "1        House Intelligence Committee Chairman Devin Nu...\n",
      "2        On Friday, it was revealed that former Milwauk...\n",
      "3        On Christmas day, Donald Trump announced that ...\n",
      "4        Pope Francis used his annual Christmas Day mes...\n",
      "                               ...                        \n",
      "23476    21st Century Wire says As 21WIRE reported earl...\n",
      "23477    21st Century Wire says It s a familiar theme. ...\n",
      "23478    Patrick Henningsen  21st Century WireRemember ...\n",
      "23479    21st Century Wire says Al Jazeera America will...\n",
      "23480    21st Century Wire says As 21WIRE predicted in ...\n",
      "Name: text, Length: 23481, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(23481, str)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "fakeArticle = pd.read_csv(\"Fake.csv\")\n",
    "fake = fakeArticle[\"text\"]\n",
    "print(fake)\n",
    "len(fake), type(fake[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241829de-9c34-4615-acb2-8a23b0407d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t eve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "59815411"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeText = ''\n",
    "for i in range(len(fake)):\n",
    "    fakeText += fake[i]\n",
    "\n",
    "print(fakeText[0:1000])\n",
    "len(fakeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e94108d4-7d07-4085-a4b7-03f3afb2d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeTokens=nltk.word_tokenize(fakeText);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5119e27e-27fe-464d-af7d-344c95daa3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11020065 ['Donald', 'Trump', 'just', 'couldn', 't', 'wish', 'all', 'Americans', 'a', 'Happy', 'New', 'Year', 'and', 'leave', 'it', 'at', 'that', '.', 'Instead', ',', 'he', 'had', 'to', 'give', 'a', 'shout', 'out', 'to', 'his', 'enemies', ',', 'haters', 'and', 'the', 'very', 'dishonest', 'fake', 'news', 'media', '.', 'The', 'former', 'reality', 'show', 'star', 'had', 'just', 'one', 'job', 'to', 'do', 'and', 'he', 'couldn', 't', 'do', 'it', '.', 'As', 'our', 'Country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', ',', 'I', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', ',', 'supporters', ',', 'enemies', ',', 'haters', ',', 'and', 'even', 'the', 'very', 'dishonest', 'Fake', 'News', 'Media', ',', 'a', 'Happy', 'and', 'Healthy', 'New', 'Year', ',', 'President', 'Angry', 'Pants', 'tweeted', '.', '2018', 'will', 'be', 'a', 'great', 'year', 'for', 'America', '!', 'As', 'our', 'Country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', ',', 'I', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', ',', 'supporters', ',', 'enemies', ',', 'haters', ',', 'and', 'even', 'the', 'very', 'dishonest', 'Fake', 'News', 'Media', ',', 'a', 'Happy', 'and', 'Healthy', 'New', 'Year', '.', '2018', 'will', 'be', 'a', 'great', 'year', 'for', 'America', '!', 'Donald', 'J.', 'Trump', '(', '@', 'realDonaldTrump', ')', 'December', '31', ',', '2017Trump', 's', 'tweet', 'went', 'down', 'about', 'as', 'welll', 'as', 'you', 'd', 'expect.What', 'kind', 'of', 'president', 'sends', 'a', 'New', 'Year', 's', 'greeting', 'like', 'this', 'despicable', ',', 'petty', ',', 'infantile', 'gibberish', '?', 'Only', 'Trump', '!', 'His', 'lack', 'of', 'decency', 'won', 't', 'even', 'allow', 'him', 'to', 'rise', 'above', 'the', 'gutter', 'long', 'enough', 'to', 'wish', 'the', 'American', 'citizens', 'a', 'happy', 'new', 'year', '!', 'Bishop', 'Talbert', 'Swan', '(', '@', 'TalbertSwan', ')', 'December', '31', ',', '2017no', 'one', 'likes', 'you', 'Calvin', '(', '@', 'calvinstowell', ')', 'December', '31', ',', '2017Your', 'impeachment', 'would', 'make', '2018', 'a', 'great', 'year', 'for', 'America', ',', 'but', 'I', 'll', 'also', 'accept', 'regaining', 'control', 'of', 'Congress', '.', 'Miranda', 'Yaver', '(', '@', 'mirandayaver', ')', 'December', '31', ',', '2017Do', 'you', 'hear', 'yourself', 'talk', '?', 'When', 'you', 'have', 'to', 'include', 'that', 'many', 'people', 'that', 'hate', 'you', 'you', 'have', 'to', 'wonder', '?', 'Why', 'do', 'the', 'they', 'all', 'hate', 'me', '?', 'Alan', 'Sandoval', '(', '@', 'AlanSandoval13', ')', 'December', '31', ',', '2017Who', 'uses', 'the', 'word', 'Haters', 'in', 'a', 'New', 'Years', 'wish', '?', '?', 'Marlene', '(', '@', 'marlene399', ')', 'December', '31', ',', '2017You', 'can', 't', 'just', 'say', 'happy', 'new', 'year', '?', 'Koren', 'pollitt', '(', '@', 'Korencarpenter', ')', 'December', '31', ',', '2017Here', 's', 'Trump', 's', 'New', 'Year', 's', 'Eve', 'tweet', 'from', '2016.Happy', 'New', 'Year', 'to', 'all', ',', 'including', 'to', 'my', 'many', 'enemies', 'and', 'those', 'who', 'have', 'fought', 'me', 'and', 'lost', 'so', 'badly', 'they', 'just', 'don', 't', 'know', 'what', 'to', 'do', '.', 'Love', '!', 'Donald', 'J.', 'Trump', '(', '@', 'realDonaldTrump', ')', 'December', '31', ',', '2016This', 'is', 'nothing', 'new', 'for', 'Trump', '.', 'He', 's', 'been', 'doing', 'this', 'for', 'years.Trump', 'has', 'directed', 'messages', 'to', 'his', 'enemies', 'and', 'haters', 'for', 'New', 'Year', 's', ',', 'Easter', ',', 'Thanksgiving', ',', 'and', 'the', 'anniversary', 'of', '9/11', '.', 'pic.twitter.com/4FPAe2KypA', 'Daniel', 'Dale', '(', '@', 'ddale8', ')', 'December', '31', ',', '2017Trump', 's', 'holiday', 'tweets', 'are', 'clearly', 'not', 'presidential.How', 'long', 'did', 'he', 'work', 'at', 'Hallmark', 'before', 'becoming', 'President', '?', 'Steven', 'Goodine', '(', '@', 'SGoodine', ')', 'December', '31', ',', '2017He', 's', 'always', 'been', 'like', 'this', '.', '.', '.', 'the', 'only', 'difference', 'is', 'that', 'in', 'the', 'last', 'few', 'years', ',', 'his', 'filter', 'has', 'been', 'breaking', 'down', '.', 'Roy', 'Schulze', '(', '@', 'thbthttt', ')', 'December', '31', ',', '2017Who', ',', 'apart', 'from', 'a', 'teenager', 'uses', 'the', 'term', 'haters', '?', 'Wendy', '(', '@', 'WendyWhistles', ')', 'December', '31', ',', '2017he', 's', 'a', 'fucking', '5', 'year', 'old', 'Who', 'Knows', '(', '@', 'rainyday80', ')', 'December', '31', ',', '2017So', ',', 'to', 'all', 'the', 'people', 'who', 'voted', 'for', 'this', 'a', 'hole', 'thinking', 'he', 'would', 'change', 'once', 'he', 'got', 'into', 'power', ',', 'you', 'were', 'wrong', '!', '70-year-old', 'men', 'don', 't', 'change', 'and', 'now', 'he', 's', 'a', 'year', 'older.Photo', 'by', 'Andrew', 'Burton/Getty', 'Images.House', 'Intelligence', 'Committee', 'Chairman', 'Devin', 'Nunes', 'is', 'going', 'to', 'have', 'a', 'bad', 'day', '.', 'He', 's', 'been', 'under', 'the', 'assumption', ',', 'like', 'many', 'of', 'us', ',', 'that', 'the', 'Christopher', 'Steele-dossier', 'was', 'what', 'prompted', 'the', 'Russia', 'investigation', 'so', 'he', 's', 'been', 'lashing', 'out', 'at', 'the', 'Department', 'of', 'Justice', 'and', 'the', 'FBI', 'in', 'order', 'to', 'protect', 'Trump', '.', 'As', 'it', 'happens', ',', 'the', 'dossier', 'is', 'not', 'what', 'started', 'the', 'investigation', ',', 'according', 'to', 'documents', 'obtained', 'by', 'the', 'New', 'York', 'Times.Former', 'Trump', 'campaign', 'adviser', 'George', 'Papadopoulos', 'was', 'drunk', 'in', 'a', 'wine', 'bar', 'when', 'he', 'revealed', 'knowledge', 'of', 'Russian', 'opposition', 'research', 'on', 'Hillary', 'Clinton.On', 'top', 'of', 'that', ',', 'Papadopoulos', 'wasn', 't', 'just', 'a', 'covfefe', 'boy', 'for', 'Trump', ',', 'as', 'his', 'administration', 'has', 'alleged', '.', 'He', 'had', 'a', 'much', 'larger', 'role', ',', 'but', 'none', 'so', 'damning', 'as', 'being', 'a', 'drunken', 'fool', 'in', 'a', 'wine', 'bar', '.', 'Coffee', 'boys', 'don', 't', 'help', 'to', 'arrange', 'a', 'New', 'York', 'meeting', 'between', 'Trump', 'and', 'President', 'Abdel', 'Fattah', 'el-Sisi', 'of', 'Egypt', 'two', 'months', 'before', 'the', 'election', '.', 'It', 'was', 'known', 'before', 'that', 'the', 'former', 'aide', 'set', 'up', 'meetings', 'with', 'world', 'leaders', 'for', 'Trump', ',', 'but', 'team', 'Trump', 'ran', 'with', 'him', 'being', 'merely', 'a', 'coffee', 'boy.In', 'May', '2016', ',', 'Papadopoulos', 'revealed', 'to', 'Australian', 'diplomat', 'Alexander', 'Downer', 'that', 'Russian', 'officials', 'were', 'shopping', 'around', 'possible', 'dirt', 'on', 'then-Democratic', 'presidential', 'nominee', 'Hillary', 'Clinton', '.', 'Exactly', 'how', 'much', 'Mr.', 'Papadopoulos', 'said', 'that', 'night', 'at', 'the', 'Kensington', 'Wine', 'Rooms', 'with', 'the', 'Australian', ',', 'Alexander', 'Downer', ',', 'is', 'unclear', ',', 'the', 'report', 'states', '.', 'But', 'two', 'months', 'later', ',', 'when', 'leaked', 'Democratic', 'emails', 'began', 'appearing', 'online', ',', 'Australian', 'officials', 'passed', 'the', 'information', 'about', 'Mr.', 'Papadopoulos', 'to', 'their', 'American', 'counterparts', ',', 'according', 'to', 'four', 'current', 'and', 'former', 'American', 'and', 'foreign', 'officials', 'with', 'direct', 'knowledge', 'of', 'the', 'Australians', 'role', '.', 'Papadopoulos', 'pleaded', 'guilty', 'to', 'lying', 'to', 'the', 'F.B.I', '.', 'and', 'is', 'now', 'a', 'cooperating', 'witness', 'with', 'Special', 'Counsel', 'Robert', 'Mueller', 's', 'team.This', 'isn', 't', 'a', 'presidency', '.', 'It', 's', 'a', 'badly', 'scripted', 'reality', 'TV', 'show.Photo', 'by', 'Win', 'McNamee/Getty', 'Images.On', 'Friday', ',', 'it', 'was', 'revealed', 'that', 'former', 'Milwaukee', 'Sheriff', 'David', 'Clarke', ',', 'who', 'was', 'being', 'considered', 'for', 'Homeland', 'Security', 'Secretary', 'in', 'Donald', 'Trump', 's', 'administration', ',', 'has', 'an', 'email', 'scandal', 'of', 'his', 'own.In', 'January', ',', 'there', 'was', 'a', 'brief', 'run-in', 'on', 'a', 'plane', 'between', 'Clarke', 'and', 'fellow', 'passenger', 'Dan', 'Black', ',', 'who', 'he', 'later', 'had', 'detained', 'by', 'the', 'police', 'for', 'no', 'reason', 'whatsoever', ',', 'except', 'that', 'maybe', 'his', 'feelings', 'were', 'hurt', '.', 'Clarke']\n"
     ]
    }
   ],
   "source": [
    "print(len(fakeTokens), fakeTokens[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ad96b40-b8d1-46b8-b3fc-663aeba9d95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['donald', 'trump', 'just', 'couldn', 't', 'wish', 'all', 'americans', 'a', 'happy', 'new', 'year', 'and', 'leave', 'it', 'at', 'that', '.', 'instead', ',', 'he', 'had', 'to', 'give', 'a', 'shout', 'out', 'to', 'his', 'enemies', ',', 'haters', 'and', 'the', 'very', 'dishonest', 'fake', 'news', 'media', '.', 'the', 'former', 'reality', 'show', 'star', 'had', 'just', 'one', 'job', 'to', 'do', 'and', 'he', 'couldn', 't', 'do', 'it', '.', 'as', 'our', 'country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', ',', 'i', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', ',', 'supporters', ',', 'enemies', ',', 'haters', ',', 'and', 'even', 'the', 'very', 'dishonest', 'fake', 'news', 'media', ',', 'a', 'happy', 'and', 'healthy', 'new', 'year', ',', 'president', 'angry', 'pants', 'tweeted', '.', '2018', 'will', 'be', 'a', 'great', 'year', 'for', 'america', '!', 'as', 'our', 'country', 'rapidly', 'grows', 'stronger', 'and', 'smarter', ',', 'i', 'want', 'to', 'wish', 'all', 'of', 'my', 'friends', ',', 'supporters', ',', 'enemies', ',', 'haters', ',', 'and', 'even', 'the', 'very', 'dishonest', 'fake', 'news', 'media', ',', 'a', 'happy', 'and', 'healthy', 'new', 'year', '.', '2018', 'will', 'be', 'a', 'great', 'year', 'for', 'america', '!', 'donald', 'j.', 'trump', '(', '@', 'realdonaldtrump', ')', 'december', '31', ',', '2017trump', 's', 'tweet', 'went', 'down', 'about', 'as', 'welll', 'as', 'you', 'd', 'expect.what', 'kind', 'of', 'president', 'sends', 'a', 'new', 'year', 's', 'greeting', 'like', 'this', 'despicable', ',', 'petty', ',', 'infantile', 'gibberish', '?', 'only', 'trump', '!', 'his', 'lack', 'of', 'decency', 'won', 't', 'even', 'allow', 'him', 'to', 'rise', 'above', 'the', 'gutter', 'long', 'enough', 'to', 'wish', 'the', 'american', 'citizens', 'a', 'happy', 'new', 'year', '!', 'bishop', 'talbert', 'swan', '(', '@', 'talbertswan', ')', 'december', '31', ',', '2017no', 'one', 'likes', 'you', 'calvin', '(', '@', 'calvinstowell', ')', 'december', '31', ',', '2017your', 'impeachment', 'would', 'make', '2018', 'a', 'great', 'year', 'for', 'america', ',', 'but', 'i', 'll', 'also', 'accept', 'regaining', 'control', 'of', 'congress', '.', 'miranda', 'yaver', '(', '@', 'mirandayaver', ')', 'december', '31', ',', '2017do', 'you', 'hear', 'yourself', 'talk', '?', 'when', 'you', 'have', 'to', 'include', 'that', 'many', 'people', 'that', 'hate', 'you', 'you', 'have', 'to', 'wonder', '?', 'why', 'do', 'the', 'they', 'all', 'hate', 'me', '?', 'alan', 'sandoval', '(', '@', 'alansandoval13', ')', 'december', '31', ',', '2017who', 'uses', 'the', 'word', 'haters', 'in', 'a', 'new', 'years', 'wish', '?', '?', 'marlene', '(', '@', 'marlene399', ')', 'december', '31', ',', '2017you', 'can', 't', 'just', 'say', 'happy', 'new', 'year', '?', 'koren', 'pollitt', '(', '@', 'korencarpenter', ')', 'december', '31', ',', '2017here', 's', 'trump', 's', 'new', 'year', 's', 'eve', 'tweet', 'from', '2016.happy', 'new', 'year', 'to', 'all', ',', 'including', 'to', 'my', 'many', 'enemies', 'and', 'those', 'who', 'have', 'fought', 'me', 'and', 'lost', 'so', 'badly', 'they', 'just', 'don', 't', 'know', 'what', 'to', 'do', '.', 'love', '!', 'donald', 'j.', 'trump', '(', '@', 'realdonaldtrump', ')', 'december', '31', ',', '2016this', 'is', 'nothing', 'new', 'for', 'trump', '.', 'he', 's', 'been', 'doing', 'this', 'for', 'years.trump', 'has', 'directed', 'messages', 'to', 'his', 'enemies', 'and', 'haters', 'for', 'new', 'year', 's', ',', 'easter', ',', 'thanksgiving', ',', 'and', 'the', 'anniversary', 'of', '9/11', '.', 'pic.twitter.com/4fpae2kypa', 'daniel', 'dale', '(', '@', 'ddale8', ')', 'december', '31', ',', '2017trump', 's', 'holiday', 'tweets', 'are', 'clearly', 'not', 'presidential.how', 'long', 'did', 'he', 'work', 'at', 'hallmark', 'before', 'becoming', 'president', '?', 'steven', 'goodine', '(', '@', 'sgoodine', ')', 'december', '31', ',', '2017he', 's', 'always', 'been', 'like', 'this', '.', '.', '.', 'the', 'only', 'difference', 'is', 'that', 'in', 'the', 'last', 'few', 'years', ',', 'his', 'filter', 'has', 'been', 'breaking', 'down', '.', 'roy', 'schulze', '(', '@', 'thbthttt', ')', 'december', '31', ',', '2017who', ',', 'apart', 'from', 'a', 'teenager', 'uses', 'the', 'term', 'haters', '?', 'wendy', '(', '@', 'wendywhistles', ')', 'december', '31', ',', '2017he', 's', 'a', 'fucking', '5', 'year', 'old', 'who', 'knows', '(', '@', 'rainyday80', ')', 'december', '31', ',', '2017so', ',', 'to', 'all', 'the', 'people', 'who', 'voted', 'for', 'this', 'a', 'hole', 'thinking', 'he', 'would', 'change', 'once', 'he', 'got', 'into', 'power', ',', 'you', 'were', 'wrong', '!', '70-year-old', 'men', 'don', 't', 'change', 'and', 'now', 'he', 's', 'a', 'year', 'older.photo', 'by', 'andrew', 'burton/getty', 'images.house', 'intelligence', 'committee', 'chairman', 'devin', 'nunes', 'is', 'going', 'to', 'have', 'a', 'bad', 'day', '.', 'he', 's', 'been', 'under', 'the', 'assumption', ',', 'like', 'many', 'of', 'us', ',', 'that', 'the', 'christopher', 'steele-dossier', 'was', 'what', 'prompted', 'the', 'russia', 'investigation', 'so', 'he', 's', 'been', 'lashing', 'out', 'at', 'the', 'department', 'of', 'justice', 'and', 'the', 'fbi', 'in', 'order', 'to', 'protect', 'trump', '.', 'as', 'it', 'happens', ',', 'the', 'dossier', 'is', 'not', 'what', 'started', 'the', 'investigation', ',', 'according', 'to', 'documents', 'obtained', 'by', 'the', 'new', 'york', 'times.former', 'trump', 'campaign', 'adviser', 'george', 'papadopoulos', 'was', 'drunk', 'in', 'a', 'wine', 'bar', 'when', 'he', 'revealed', 'knowledge', 'of', 'russian', 'opposition', 'research', 'on', 'hillary', 'clinton.on', 'top', 'of', 'that', ',', 'papadopoulos', 'wasn', 't', 'just', 'a', 'covfefe', 'boy', 'for', 'trump', ',', 'as', 'his', 'administration', 'has', 'alleged', '.', 'he', 'had', 'a', 'much', 'larger', 'role', ',', 'but', 'none', 'so', 'damning', 'as', 'being', 'a', 'drunken', 'fool', 'in', 'a', 'wine', 'bar', '.', 'coffee', 'boys', 'don', 't', 'help', 'to', 'arrange', 'a', 'new', 'york', 'meeting', 'between', 'trump', 'and', 'president', 'abdel', 'fattah', 'el-sisi', 'of', 'egypt', 'two', 'months', 'before', 'the', 'election', '.', 'it', 'was', 'known', 'before', 'that', 'the', 'former', 'aide', 'set', 'up', 'meetings', 'with', 'world', 'leaders', 'for', 'trump', ',', 'but', 'team', 'trump', 'ran', 'with', 'him', 'being', 'merely', 'a', 'coffee', 'boy.in', 'may', '2016', ',', 'papadopoulos', 'revealed', 'to', 'australian', 'diplomat', 'alexander', 'downer', 'that', 'russian', 'officials', 'were', 'shopping', 'around', 'possible', 'dirt', 'on', 'then-democratic', 'presidential', 'nominee', 'hillary', 'clinton', '.', 'exactly', 'how', 'much', 'mr.', 'papadopoulos', 'said', 'that', 'night', 'at', 'the', 'kensington', 'wine', 'rooms', 'with', 'the', 'australian', ',', 'alexander', 'downer', ',', 'is', 'unclear', ',', 'the', 'report', 'states', '.', 'but', 'two', 'months', 'later', ',', 'when', 'leaked', 'democratic', 'emails', 'began', 'appearing', 'online', ',', 'australian', 'officials', 'passed', 'the', 'information', 'about', 'mr.', 'papadopoulos', 'to', 'their', 'american', 'counterparts', ',', 'according', 'to', 'four', 'current', 'and', 'former', 'american', 'and', 'foreign', 'officials', 'with', 'direct', 'knowledge', 'of', 'the', 'australians', 'role', '.', 'papadopoulos', 'pleaded', 'guilty', 'to', 'lying', 'to', 'the', 'f.b.i', '.', 'and', 'is', 'now', 'a', 'cooperating', 'witness', 'with', 'special', 'counsel', 'robert', 'mueller', 's', 'team.this', 'isn', 't', 'a', 'presidency', '.', 'it', 's', 'a', 'badly', 'scripted', 'reality', 'tv', 'show.photo', 'by', 'win', 'mcnamee/getty', 'images.on', 'friday', ',', 'it', 'was', 'revealed', 'that', 'former', 'milwaukee', 'sheriff', 'david', 'clarke', ',', 'who', 'was', 'being', 'considered', 'for', 'homeland', 'security', 'secretary', 'in', 'donald', 'trump', 's', 'administration', ',', 'has', 'an', 'email', 'scandal', 'of', 'his', 'own.in', 'january', ',', 'there', 'was', 'a', 'brief', 'run-in', 'on', 'a', 'plane', 'between', 'clarke', 'and', 'fellow', 'passenger', 'dan', 'black', ',', 'who', 'he', 'later', 'had', 'detained', 'by', 'the', 'police', 'for', 'no', 'reason', 'whatsoever', ',', 'except', 'that', 'maybe', 'his', 'feelings', 'were', 'hurt', '.', 'clarke']\n"
     ]
    }
   ],
   "source": [
    "fakeWords = [w.lower() for w in fakeTokens]\n",
    "print(fakeWords[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2b77be-5c62-46cd-890f-cdb290a0c181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump\n",
      "Trump just\n",
      "just couldn\n",
      "couldn t\n",
      "t wish\n",
      "wish all\n",
      "all Americans\n",
      "Americans a\n",
      "a Happy\n",
      "Happy New\n",
      "New Year\n",
      "Year and\n",
      "and leave\n",
      "leave it\n",
      "it at\n",
      "at that\n",
      "that .\n",
      ". Instead\n",
      "Instead ,\n",
      ", he\n",
      "he had\n",
      "had to\n",
      "to give\n",
      "give a\n",
      "a shout\n",
      "shout out\n",
      "out to\n",
      "to his\n",
      "his enemies\n",
      "enemies ,\n",
      ", haters\n",
      "haters and\n",
      "and the\n",
      "the very\n",
      "very dishonest\n",
      "dishonest fake\n",
      "fake news\n",
      "news media\n",
      "media .\n",
      ". The\n",
      "The former\n",
      "former reality\n",
      "reality show\n",
      "show star\n",
      "star had\n",
      "had just\n",
      "just one\n",
      "one job\n",
      "job to\n",
      "to do\n",
      "do and\n",
      "and he\n",
      "he couldn\n",
      "couldn t\n",
      "t do\n",
      "do it\n",
      "it .\n",
      ". As\n",
      "As our\n",
      "our Country\n",
      "Country rapidly\n",
      "rapidly grows\n",
      "grows stronger\n",
      "stronger and\n",
      "and smarter\n",
      "smarter ,\n",
      ", I\n",
      "I want\n",
      "want to\n",
      "to wish\n",
      "wish all\n",
      "all of\n",
      "of my\n",
      "my friends\n",
      "friends ,\n",
      ", supporters\n",
      "supporters ,\n",
      ", enemies\n",
      "enemies ,\n",
      ", haters\n",
      "haters ,\n",
      ", and\n",
      "and even\n",
      "even the\n",
      "the very\n",
      "very dishonest\n",
      "dishonest Fake\n",
      "Fake News\n",
      "News Media\n",
      "Media ,\n",
      ", a\n",
      "a Happy\n",
      "Happy and\n",
      "and Healthy\n",
      "Healthy New\n",
      "New Year\n",
      "Year ,\n",
      ", President\n",
      "President Angry\n",
      "Angry Pants\n",
      "Pants tweeted\n",
      "tweeted .\n",
      ". 2018\n",
      "2018 will\n",
      "will be\n",
      "be a\n",
      "a great\n",
      "great year\n",
      "year for\n",
      "for America\n",
      "America !\n",
      "! As\n",
      "As our\n",
      "our Country\n",
      "Country rapidly\n",
      "rapidly grows\n",
      "grows stronger\n",
      "stronger and\n",
      "and smarter\n",
      "smarter ,\n",
      ", I\n",
      "I want\n",
      "want to\n",
      "to wish\n",
      "wish all\n",
      "all of\n",
      "of my\n",
      "my friends\n",
      "friends ,\n",
      ", supporters\n",
      "supporters ,\n",
      ", enemies\n",
      "enemies ,\n",
      ", haters\n",
      "haters ,\n",
      ", and\n",
      "and even\n",
      "even the\n",
      "the very\n",
      "very dishonest\n",
      "dishonest Fake\n",
      "Fake News\n",
      "News Media\n",
      "Media ,\n",
      ", a\n",
      "a Happy\n",
      "Happy and\n",
      "and Healthy\n",
      "Healthy New\n",
      "New Year\n",
      "Year .\n",
      ". 2018\n",
      "2018 will\n",
      "will be\n",
      "be a\n",
      "a great\n",
      "great year\n",
      "year for\n",
      "for America\n",
      "America !\n",
      "! Donald\n",
      "Donald J.\n",
      "J. Trump\n",
      "Trump (\n",
      "( @\n",
      "@ realDonaldTrump\n",
      "realDonaldTrump )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Trump\n",
      "2017Trump s\n",
      "s tweet\n",
      "tweet went\n",
      "went down\n",
      "down about\n",
      "about as\n",
      "as welll\n",
      "welll as\n",
      "as you\n",
      "you d\n",
      "d expect.What\n",
      "expect.What kind\n",
      "kind of\n",
      "of president\n",
      "president sends\n",
      "sends a\n",
      "a New\n",
      "New Year\n",
      "Year s\n",
      "s greeting\n",
      "greeting like\n",
      "like this\n",
      "this despicable\n",
      "despicable ,\n",
      ", petty\n",
      "petty ,\n",
      ", infantile\n",
      "infantile gibberish\n",
      "gibberish ?\n",
      "? Only\n",
      "Only Trump\n",
      "Trump !\n",
      "! His\n",
      "His lack\n",
      "lack of\n",
      "of decency\n",
      "decency won\n",
      "won t\n",
      "t even\n",
      "even allow\n",
      "allow him\n",
      "him to\n",
      "to rise\n",
      "rise above\n",
      "above the\n",
      "the gutter\n",
      "gutter long\n",
      "long enough\n",
      "enough to\n",
      "to wish\n",
      "wish the\n",
      "the American\n",
      "American citizens\n",
      "citizens a\n",
      "a happy\n",
      "happy new\n",
      "new year\n",
      "year !\n",
      "! Bishop\n",
      "Bishop Talbert\n",
      "Talbert Swan\n",
      "Swan (\n",
      "( @\n",
      "@ TalbertSwan\n",
      "TalbertSwan )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017no\n",
      "2017no one\n",
      "one likes\n",
      "likes you\n",
      "you Calvin\n",
      "Calvin (\n",
      "( @\n",
      "@ calvinstowell\n",
      "calvinstowell )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Your\n",
      "2017Your impeachment\n",
      "impeachment would\n",
      "would make\n",
      "make 2018\n",
      "2018 a\n",
      "a great\n",
      "great year\n",
      "year for\n",
      "for America\n",
      "America ,\n",
      ", but\n",
      "but I\n",
      "I ll\n",
      "ll also\n",
      "also accept\n",
      "accept regaining\n",
      "regaining control\n",
      "control of\n",
      "of Congress\n",
      "Congress .\n",
      ". Miranda\n",
      "Miranda Yaver\n",
      "Yaver (\n",
      "( @\n",
      "@ mirandayaver\n",
      "mirandayaver )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Do\n",
      "2017Do you\n",
      "you hear\n",
      "hear yourself\n",
      "yourself talk\n",
      "talk ?\n",
      "? When\n",
      "When you\n",
      "you have\n",
      "have to\n",
      "to include\n",
      "include that\n",
      "that many\n",
      "many people\n",
      "people that\n",
      "that hate\n",
      "hate you\n",
      "you you\n",
      "you have\n",
      "have to\n",
      "to wonder\n",
      "wonder ?\n",
      "? Why\n",
      "Why do\n",
      "do the\n",
      "the they\n",
      "they all\n",
      "all hate\n",
      "hate me\n",
      "me ?\n",
      "? Alan\n",
      "Alan Sandoval\n",
      "Sandoval (\n",
      "( @\n",
      "@ AlanSandoval13\n",
      "AlanSandoval13 )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Who\n",
      "2017Who uses\n",
      "uses the\n",
      "the word\n",
      "word Haters\n",
      "Haters in\n",
      "in a\n",
      "a New\n",
      "New Years\n",
      "Years wish\n",
      "wish ?\n",
      "? ?\n",
      "? Marlene\n",
      "Marlene (\n",
      "( @\n",
      "@ marlene399\n",
      "marlene399 )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017You\n",
      "2017You can\n",
      "can t\n",
      "t just\n",
      "just say\n",
      "say happy\n",
      "happy new\n",
      "new year\n",
      "year ?\n",
      "? Koren\n",
      "Koren pollitt\n",
      "pollitt (\n",
      "( @\n",
      "@ Korencarpenter\n",
      "Korencarpenter )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Here\n",
      "2017Here s\n",
      "s Trump\n",
      "Trump s\n",
      "s New\n",
      "New Year\n",
      "Year s\n",
      "s Eve\n",
      "Eve tweet\n",
      "tweet from\n",
      "from 2016.Happy\n",
      "2016.Happy New\n",
      "New Year\n",
      "Year to\n",
      "to all\n",
      "all ,\n",
      ", including\n",
      "including to\n",
      "to my\n",
      "my many\n",
      "many enemies\n",
      "enemies and\n",
      "and those\n",
      "those who\n",
      "who have\n",
      "have fought\n",
      "fought me\n",
      "me and\n",
      "and lost\n",
      "lost so\n",
      "so badly\n",
      "badly they\n",
      "they just\n",
      "just don\n",
      "don t\n",
      "t know\n",
      "know what\n",
      "what to\n",
      "to do\n",
      "do .\n",
      ". Love\n",
      "Love !\n",
      "! Donald\n",
      "Donald J.\n",
      "J. Trump\n",
      "Trump (\n",
      "( @\n",
      "@ realDonaldTrump\n",
      "realDonaldTrump )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2016This\n",
      "2016This is\n",
      "is nothing\n",
      "nothing new\n",
      "new for\n",
      "for Trump\n",
      "Trump .\n",
      ". He\n",
      "He s\n",
      "s been\n",
      "been doing\n",
      "doing this\n",
      "this for\n",
      "for years.Trump\n",
      "years.Trump has\n",
      "has directed\n",
      "directed messages\n",
      "messages to\n",
      "to his\n",
      "his enemies\n",
      "enemies and\n",
      "and haters\n",
      "haters for\n",
      "for New\n",
      "New Year\n",
      "Year s\n",
      "s ,\n",
      ", Easter\n",
      "Easter ,\n",
      ", Thanksgiving\n",
      "Thanksgiving ,\n",
      ", and\n",
      "and the\n",
      "the anniversary\n",
      "anniversary of\n",
      "of 9/11\n",
      "9/11 .\n",
      ". pic.twitter.com/4FPAe2KypA\n",
      "pic.twitter.com/4FPAe2KypA Daniel\n",
      "Daniel Dale\n",
      "Dale (\n",
      "( @\n",
      "@ ddale8\n",
      "ddale8 )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Trump\n",
      "2017Trump s\n",
      "s holiday\n",
      "holiday tweets\n",
      "tweets are\n",
      "are clearly\n",
      "clearly not\n",
      "not presidential.How\n",
      "presidential.How long\n",
      "long did\n",
      "did he\n",
      "he work\n",
      "work at\n",
      "at Hallmark\n",
      "Hallmark before\n",
      "before becoming\n",
      "becoming President\n",
      "President ?\n",
      "? Steven\n",
      "Steven Goodine\n",
      "Goodine (\n",
      "( @\n",
      "@ SGoodine\n",
      "SGoodine )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017He\n",
      "2017He s\n",
      "s always\n",
      "always been\n",
      "been like\n",
      "like this\n",
      "this .\n",
      ". .\n",
      ". .\n",
      ". the\n",
      "the only\n",
      "only difference\n",
      "difference is\n",
      "is that\n",
      "that in\n",
      "in the\n",
      "the last\n",
      "last few\n",
      "few years\n",
      "years ,\n",
      ", his\n",
      "his filter\n",
      "filter has\n",
      "has been\n",
      "been breaking\n",
      "breaking down\n",
      "down .\n",
      ". Roy\n",
      "Roy Schulze\n",
      "Schulze (\n",
      "( @\n",
      "@ thbthttt\n",
      "thbthttt )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017Who\n",
      "2017Who ,\n",
      ", apart\n",
      "apart from\n",
      "from a\n",
      "a teenager\n",
      "teenager uses\n",
      "uses the\n",
      "the term\n",
      "term haters\n",
      "haters ?\n",
      "? Wendy\n",
      "Wendy (\n",
      "( @\n",
      "@ WendyWhistles\n",
      "WendyWhistles )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017he\n",
      "2017he s\n",
      "s a\n",
      "a fucking\n",
      "fucking 5\n",
      "5 year\n",
      "year old\n",
      "old Who\n",
      "Who Knows\n",
      "Knows (\n",
      "( @\n",
      "@ rainyday80\n",
      "rainyday80 )\n",
      ") December\n",
      "December 31\n",
      "31 ,\n",
      ", 2017So\n",
      "2017So ,\n",
      ", to\n",
      "to all\n",
      "all the\n",
      "the people\n",
      "people who\n",
      "who voted\n",
      "voted for\n",
      "for this\n",
      "this a\n",
      "a hole\n",
      "hole thinking\n",
      "thinking he\n",
      "he would\n",
      "would change\n",
      "change once\n",
      "once he\n",
      "he got\n",
      "got into\n",
      "into power\n",
      "power ,\n",
      ", you\n",
      "you were\n",
      "were wrong\n",
      "wrong !\n",
      "! 70-year-old\n",
      "70-year-old men\n",
      "men don\n",
      "don t\n",
      "t change\n",
      "change and\n",
      "and now\n",
      "now he\n",
      "he s\n",
      "s a\n",
      "a year\n",
      "year older.Photo\n",
      "older.Photo by\n",
      "by Andrew\n",
      "Andrew Burton/Getty\n",
      "Burton/Getty Images.House\n",
      "Images.House Intelligence\n",
      "Intelligence Committee\n",
      "Committee Chairman\n",
      "Chairman Devin\n",
      "Devin Nunes\n",
      "Nunes is\n",
      "is going\n",
      "going to\n",
      "to have\n",
      "have a\n",
      "a bad\n",
      "bad day\n",
      "day .\n",
      ". He\n",
      "He s\n",
      "s been\n",
      "been under\n",
      "under the\n",
      "the assumption\n",
      "assumption ,\n",
      ", like\n",
      "like many\n",
      "many of\n",
      "of us\n",
      "us ,\n",
      ", that\n",
      "that the\n",
      "the Christopher\n",
      "Christopher Steele-dossier\n",
      "Steele-dossier was\n",
      "was what\n",
      "what prompted\n",
      "prompted the\n",
      "the Russia\n",
      "Russia investigation\n",
      "investigation so\n",
      "so he\n",
      "he s\n",
      "s been\n",
      "been lashing\n",
      "lashing out\n",
      "out at\n",
      "at the\n",
      "the Department\n",
      "Department of\n",
      "of Justice\n",
      "Justice and\n",
      "and the\n",
      "the FBI\n",
      "FBI in\n",
      "in order\n",
      "order to\n",
      "to protect\n",
      "protect Trump\n",
      "Trump .\n",
      ". As\n",
      "As it\n",
      "it happens\n",
      "happens ,\n",
      ", the\n",
      "the dossier\n",
      "dossier is\n",
      "is not\n",
      "not what\n",
      "what started\n",
      "started the\n",
      "the investigation\n",
      "investigation ,\n",
      ", according\n",
      "according to\n",
      "to documents\n",
      "documents obtained\n",
      "obtained by\n",
      "by the\n",
      "the New\n",
      "New York\n",
      "York Times.Former\n",
      "Times.Former Trump\n",
      "Trump campaign\n",
      "campaign adviser\n",
      "adviser George\n",
      "George Papadopoulos\n",
      "Papadopoulos was\n",
      "was drunk\n",
      "drunk in\n",
      "in a\n",
      "a wine\n",
      "wine bar\n",
      "bar when\n",
      "when he\n",
      "he revealed\n",
      "revealed knowledge\n",
      "knowledge of\n",
      "of Russian\n",
      "Russian opposition\n",
      "opposition research\n",
      "research on\n",
      "on Hillary\n",
      "Hillary Clinton.On\n",
      "Clinton.On top\n",
      "top of\n",
      "of that\n",
      "that ,\n",
      ", Papadopoulos\n",
      "Papadopoulos wasn\n",
      "wasn t\n",
      "t just\n",
      "just a\n",
      "a covfefe\n",
      "covfefe boy\n",
      "boy for\n",
      "for Trump\n",
      "Trump ,\n",
      ", as\n",
      "as his\n",
      "his administration\n",
      "administration has\n",
      "has alleged\n",
      "alleged .\n",
      ". He\n",
      "He had\n",
      "had a\n",
      "a much\n",
      "much larger\n",
      "larger role\n",
      "role ,\n",
      ", but\n",
      "but none\n",
      "none so\n",
      "so damning\n",
      "damning as\n",
      "as being\n",
      "being a\n",
      "a drunken\n",
      "drunken fool\n",
      "fool in\n",
      "in a\n",
      "a wine\n",
      "wine bar\n",
      "bar .\n",
      ". Coffee\n",
      "Coffee boys\n",
      "boys don\n",
      "don t\n",
      "t help\n",
      "help to\n",
      "to arrange\n",
      "arrange a\n",
      "a New\n",
      "New York\n",
      "York meeting\n",
      "meeting between\n",
      "between Trump\n",
      "Trump and\n",
      "and President\n",
      "President Abdel\n",
      "Abdel Fattah\n",
      "Fattah el-Sisi\n",
      "el-Sisi of\n",
      "of Egypt\n",
      "Egypt two\n",
      "two months\n",
      "months before\n",
      "before the\n",
      "the election\n",
      "election .\n",
      ". It\n",
      "It was\n",
      "was known\n",
      "known before\n",
      "before that\n",
      "that the\n",
      "the former\n",
      "former aide\n",
      "aide set\n",
      "set up\n",
      "up meetings\n",
      "meetings with\n",
      "with world\n",
      "world leaders\n",
      "leaders for\n",
      "for Trump\n",
      "Trump ,\n",
      ", but\n",
      "but team\n",
      "team Trump\n",
      "Trump ran\n",
      "ran with\n",
      "with him\n",
      "him being\n",
      "being merely\n",
      "merely a\n",
      "a coffee\n",
      "coffee boy.In\n",
      "boy.In May\n",
      "May 2016\n",
      "2016 ,\n",
      ", Papadopoulos\n",
      "Papadopoulos revealed\n",
      "revealed to\n",
      "to Australian\n",
      "Australian diplomat\n",
      "diplomat Alexander\n",
      "Alexander Downer\n",
      "Downer that\n",
      "that Russian\n",
      "Russian officials\n",
      "officials were\n",
      "were shopping\n",
      "shopping around\n",
      "around possible\n",
      "possible dirt\n",
      "dirt on\n",
      "on then-Democratic\n",
      "then-Democratic presidential\n",
      "presidential nominee\n",
      "nominee Hillary\n",
      "Hillary Clinton\n",
      "Clinton .\n",
      ". Exactly\n",
      "Exactly how\n",
      "how much\n",
      "much Mr.\n",
      "Mr. Papadopoulos\n",
      "Papadopoulos said\n",
      "said that\n",
      "that night\n",
      "night at\n",
      "at the\n",
      "the Kensington\n",
      "Kensington Wine\n",
      "Wine Rooms\n",
      "Rooms with\n",
      "with the\n",
      "the Australian\n",
      "Australian ,\n",
      ", Alexander\n",
      "Alexander Downer\n",
      "Downer ,\n",
      ", is\n",
      "is unclear\n",
      "unclear ,\n",
      ", the\n",
      "the report\n",
      "report states\n",
      "states .\n",
      ". But\n",
      "But two\n",
      "two months\n",
      "months later\n",
      "later ,\n",
      ", when\n",
      "when leaked\n",
      "leaked Democratic\n",
      "Democratic emails\n",
      "emails began\n",
      "began appearing\n",
      "appearing online\n",
      "online ,\n",
      ", Australian\n",
      "Australian officials\n",
      "officials passed\n",
      "passed the\n",
      "the information\n",
      "information about\n",
      "about Mr.\n",
      "Mr. Papadopoulos\n",
      "Papadopoulos to\n",
      "to their\n",
      "their American\n",
      "American counterparts\n",
      "counterparts ,\n",
      ", according\n",
      "according to\n",
      "to four\n",
      "four current\n",
      "current and\n",
      "and former\n",
      "former American\n",
      "American and\n",
      "and foreign\n",
      "foreign officials\n",
      "officials with\n",
      "with direct\n",
      "direct knowledge\n",
      "knowledge of\n",
      "of the\n",
      "the Australians\n",
      "Australians role\n",
      "role .\n",
      ". Papadopoulos\n",
      "Papadopoulos pleaded\n",
      "pleaded guilty\n",
      "guilty to\n",
      "to lying\n",
      "lying to\n",
      "to the\n",
      "the F.B.I\n",
      "F.B.I .\n",
      ". and\n",
      "and is\n",
      "is now\n",
      "now a\n",
      "a cooperating\n",
      "cooperating witness\n",
      "witness with\n",
      "with Special\n",
      "Special Counsel\n",
      "Counsel Robert\n",
      "Robert Mueller\n",
      "Mueller s\n",
      "s team.This\n",
      "team.This isn\n",
      "isn t\n",
      "t a\n",
      "a presidency\n",
      "presidency .\n",
      ". It\n",
      "It s\n",
      "s a\n",
      "a badly\n",
      "badly scripted\n",
      "scripted reality\n",
      "reality TV\n",
      "TV show.Photo\n",
      "show.Photo by\n",
      "by Win\n",
      "Win McNamee/Getty\n",
      "McNamee/Getty Images.On\n",
      "Images.On Friday\n",
      "Friday ,\n",
      ", it\n",
      "it was\n",
      "was revealed\n",
      "revealed that\n",
      "that former\n",
      "former Milwaukee\n",
      "Milwaukee Sheriff\n",
      "Sheriff David\n",
      "David Clarke\n",
      "Clarke ,\n",
      ", who\n",
      "who was\n",
      "was being\n",
      "being considered\n",
      "considered for\n",
      "for Homeland\n",
      "Homeland Security\n",
      "Security Secretary\n",
      "Secretary in\n",
      "in Donald\n",
      "Donald Trump\n",
      "Trump s\n",
      "s administration\n",
      "administration ,\n",
      ", has\n",
      "has an\n",
      "an email\n",
      "email scandal\n",
      "scandal of\n",
      "of his\n",
      "his own.In\n",
      "own.In January\n",
      "January ,\n",
      ", there\n",
      "there was\n",
      "was a\n",
      "a brief\n",
      "brief run-in\n",
      "run-in on\n",
      "on a\n",
      "a plane\n",
      "plane between\n",
      "between Clarke\n",
      "Clarke and\n",
      "and fellow\n",
      "fellow passenger\n",
      "passenger Dan\n",
      "Dan Black\n",
      "Black ,\n",
      ", who\n",
      "who he\n",
      "he later\n",
      "later had\n",
      "had detained\n",
      "detained by\n",
      "by the\n",
      "the police\n",
      "police for\n",
      "for no\n",
      "no reason\n",
      "reason whatsoever\n",
      "whatsoever ,\n",
      ", except\n",
      "except that\n",
      "that maybe\n",
      "maybe his\n",
      "his feelings\n",
      "feelings were\n",
      "were hurt\n",
      "hurt .\n",
      ". Clarke\n",
      "Clarke messaged\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "fakebigrams =[\" \".join(w) for w in ngrams(fakeTokens, 2)]\n",
    "for i in range(1000):\n",
    "  print(fakebigrams[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22a55d1c-ba39-47f5-8650-9e9cd467e59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1752164"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fakebigramfdist = FreqDist(fakebigrams) \n",
    "len(fakebigramfdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5556b9ee-86e6-4f4d-86bb-b79b725ef254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('of the', 52884)\n",
      "(', and', 37963)\n",
      "('in the', 37195)\n",
      "(', the', 28329)\n",
      "('to the', 26851)\n",
      "('. The', 25327)\n",
      "('( @', 17770)\n",
      "('on the', 17762)\n",
      "('to be', 16212)\n",
      "(', but', 15879)\n",
      "('for the', 15308)\n",
      "('that the', 14247)\n",
      "('and the', 13302)\n",
      "('Trump s', 13235)\n",
      "('. He', 13073)\n",
      "('. It', 12928)\n",
      "('. I', 12820)\n",
      "('Donald Trump', 12092)\n",
      "('at the', 11835)\n",
      "('with the', 11322)\n",
      "(', he', 11319)\n",
      "('is a', 10800)\n",
      "('in a', 10400)\n",
      "(', a', 10316)\n",
      "('it s', 10182)\n",
      "('from the', 9755)\n",
      "('by the', 9640)\n",
      "('of a', 9254)\n",
      "('that he', 9107)\n",
      "(', it', 9032)\n",
      "(', which', 8964)\n",
      "('don t', 8810)\n",
      "(', who', 8589)\n",
      "('as a', 8290)\n",
      "('. We', 7989)\n",
      "('. And', 7899)\n",
      "('has been', 7699)\n",
      "(', I', 7627)\n",
      "('. In', 7531)\n",
      "('. This', 7203)\n",
      "('. But', 7079)\n",
      "('going to', 6797)\n",
      "('have been', 6795)\n",
      "('. They', 6784)\n",
      "('It s', 6770)\n",
      "('is the', 6727)\n",
      "('Hillary Clinton', 6659)\n",
      "(', as', 6618)\n",
      "('the United', 6618)\n",
      "('about the', 6384)\n"
     ]
    }
   ],
   "source": [
    "topfakebigrams = fakebigramfdist.most_common(50)\n",
    "\n",
    "for pair in topfakebigrams:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8afa459-d796-4a4f-97dc-8c28717e5d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saisrivishwanath/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk_stops = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bad733b-a2b1-4e07-a613-44b1566a91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words, stop words and content words in fake article are  11020065 4411329 5448661\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "fakestopw = [w for w in fakeWords if w in nltk_stops]\n",
    "fakecontentw = [w for w in fakeWords if not(alpha_filter(w)) and w not in nltk_stops]\n",
    "print(\"Total words, stop words and content words in fake article are \", len(fakeWords), len(fakestopw), len(fakecontentw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bbd684cf-4181-4ba2-93cc-cd87798b8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152 181204\n",
      "<FreqDist with 181204 samples and 5448661 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "fakestopwfdist = FreqDist(fakestopw) \n",
    "fakecontentwfdist = FreqDist(fakecontentw) \n",
    "print(len(fakestopwfdist), len(fakecontentwfdist))\n",
    "print(fakecontentwfdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "464ae71e-4604-4c05-b97a-cc49a8657918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 50 stop words\n",
      "('the', 527326)\n",
      "('to', 289638)\n",
      "('of', 235777)\n",
      "('and', 224519)\n",
      "('a', 210576)\n",
      "('in', 164579)\n",
      "('that', 150256)\n",
      "('s', 129631)\n",
      "('is', 110825)\n",
      "('for', 92017)\n",
      "('on', 81064)\n",
      "('it', 78808)\n",
      "('he', 77607)\n",
      "('was', 67786)\n",
      "('with', 62876)\n",
      "('his', 58036)\n",
      "('as', 55766)\n",
      "('this', 55177)\n",
      "('be', 48557)\n",
      "('by', 47334)\n",
      "('not', 46802)\n",
      "('are', 46336)\n",
      "('have', 46027)\n",
      "('they', 45176)\n",
      "('i', 43726)\n",
      "('who', 42270)\n",
      "('has', 42252)\n",
      "('at', 41516)\n",
      "('from', 40740)\n",
      "('t', 40531)\n",
      "('you', 40191)\n",
      "('we', 38002)\n",
      "('an', 34808)\n",
      "('about', 32826)\n",
      "('but', 31403)\n",
      "('their', 30202)\n",
      "('she', 25745)\n",
      "('her', 25701)\n",
      "('or', 24671)\n",
      "('what', 24475)\n",
      "('all', 24430)\n",
      "('will', 24044)\n",
      "('been', 22991)\n",
      "('out', 22917)\n",
      "('more', 22371)\n",
      "('if', 21881)\n",
      "('were', 21607)\n",
      "('when', 21044)\n",
      "('can', 20690)\n",
      "('had', 20425)\n",
      "top 50 content words\n",
      "('trump', 74038)\n",
      "('said', 31125)\n",
      "('people', 25997)\n",
      "('president', 25591)\n",
      "('would', 23457)\n",
      "('one', 22881)\n",
      "('clinton', 18074)\n",
      "('obama', 17865)\n",
      "('like', 17643)\n",
      "('donald', 16188)\n",
      "('also', 15243)\n",
      "('new', 14173)\n",
      "('us', 13914)\n",
      "('even', 13666)\n",
      "('hillary', 13549)\n",
      "('news', 13406)\n",
      "('time', 12782)\n",
      "('white', 12762)\n",
      "('state', 12532)\n",
      "('via', 11348)\n",
      "('media', 11050)\n",
      "('get', 10705)\n",
      "('america', 10613)\n",
      "('campaign', 10561)\n",
      "('house', 10559)\n",
      "('know', 10287)\n",
      "('could', 10223)\n",
      "('first', 10016)\n",
      "('american', 9937)\n",
      "('going', 9745)\n",
      "('many', 9695)\n",
      "('image', 9622)\n",
      "('states', 9522)\n",
      "('make', 9143)\n",
      "('told', 9103)\n",
      "('republican', 8928)\n",
      "('right', 8896)\n",
      "('country', 8684)\n",
      "('made', 8666)\n",
      "('government', 8600)\n",
      "('police', 8564)\n",
      "('say', 8551)\n",
      "('way', 8457)\n",
      "('back', 8397)\n",
      "('think', 8358)\n",
      "('two', 8308)\n",
      "('years', 8259)\n",
      "('video', 8068)\n",
      "('election', 8019)\n",
      "('united', 7974)\n"
     ]
    }
   ],
   "source": [
    "topfakestopw = fakestopwfdist.most_common(50)\n",
    "topfakecontentw = fakecontentwfdist.most_common(50)\n",
    "print(\"top 50 stop words\")\n",
    "for pair in topfakestopw:\n",
    "    print (pair)\n",
    "\n",
    "print(\"top 50 content words\")\n",
    "for pair in topfakecontentw:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "69d4b5e3-c9d2-4226-8cdf-07487dc5a81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.collocations import *\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "def alpha_filter(w):\n",
    "  # pattern to match word of non-alphabetical characters\n",
    "  pattern = re.compile('^[^a-z]+$')\n",
    "  if (pattern.match(w)):\n",
    "    return True\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(fakeWords)\n",
    "scored1 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "finder.apply_word_filter(alpha_filter)\n",
    "scored2 = finder.score_ngrams(bigram_measures.raw_freq)\n",
    "finder.apply_word_filter(lambda w: w in nltk_stops)\n",
    "scored3 = finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e1215f5-710b-4b51-914a-9aff4dedb2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 Bigrams\n",
      "(('of', 'the'), 0.004887629973144442)\n",
      "(('in', 'the'), 0.003538817602255522)\n",
      "((',', 'and'), 0.0034519760092159165)\n",
      "((',', 'the'), 0.002693178307024505)\n",
      "(('to', 'the'), 0.0025075169701812105)\n",
      "(('.', 'the'), 0.0023100589697066217)\n",
      "(('on', 'the'), 0.001688828514169381)\n",
      "(('(', '@'), 0.0016125131748315458)\n",
      "(('it', 's'), 0.001543548064371671)\n",
      "(('to', 'be'), 0.001488013001738193)\n",
      "((',', 'but'), 0.0014498099602860781)\n",
      "(('for', 'the'), 0.0014387392451859404)\n",
      "(('that', 'the'), 0.0013070703303474163)\n",
      "(('and', 'the'), 0.0012742211593125812)\n",
      "(('trump', 's'), 0.00120552827955189)\n",
      "(('.', 'he'), 0.0011901926168312075)\n",
      "(('.', 'it'), 0.001177125543270389)\n",
      "(('.', 'i'), 0.001163967726143176)\n",
      "(('at', 'the'), 0.001148269089156915)\n",
      "(('donald', 'trump'), 0.0011043492030219423)\n",
      "(('with', 'the'), 0.0010651479823394871)\n",
      "((',', 'he'), 0.0010394675530498233)\n",
      "(('in', 'a'), 0.0010280338636841071)\n",
      "(('is', 'a'), 0.0009880159509041008)\n",
      "((',', 'a'), 0.0009442775519019171)\n",
      "(('from', 'the'), 0.0009253121465254516)\n",
      "(('by', 'the'), 0.0009186878661786477)\n",
      "(('don', 't'), 0.0008626990857131968)\n",
      "((',', 'it'), 0.0008465467308949629)\n",
      "(('of', 'a'), 0.0008436429367703367)\n",
      "(('that', 'he'), 0.00083021288894394)\n",
      "((',', 'which'), 0.0008139697905593116)\n",
      "(('as', 'a'), 0.0008065288181149567)\n",
      "((',', 'who'), 0.0007832984651179462)\n",
      "(('he', 's'), 0.0007372007333895036)\n",
      "(('.', 'and'), 0.0007292152995467812)\n",
      "(('.', 'we'), 0.0007279448896172573)\n",
      "(('has', 'been'), 0.000700631076132491)\n",
      "((',', 'i'), 0.0006953679492816059)\n",
      "(('.', 'in'), 0.000686021361942965)\n",
      "(('this', 'is'), 0.00067222833985099)\n",
      "(('that', 's'), 0.0006640614188754785)\n",
      "(('.', 'this'), 0.0006573463949622802)\n",
      "(('.', 'but'), 0.0006455497313309858)\n",
      "(('one', 'of'), 0.0006411940401440464)\n",
      "(('he', 'was'), 0.0006370198360898961)\n",
      "(('it', 'was'), 0.0006314844785398271)\n",
      "(('is', 'the'), 0.0006267658130873093)\n",
      "(('the', 'united'), 0.0006241342496618667)\n",
      "(('going', 'to'), 0.0006199600456077165)\n",
      "Top 50 Bigrams after removing punctuations\n",
      "(('of', 'the'), 0.004887629973144442)\n",
      "(('in', 'the'), 0.003538817602255522)\n",
      "(('to', 'the'), 0.0025075169701812105)\n",
      "(('on', 'the'), 0.001688828514169381)\n",
      "(('it', 's'), 0.001543548064371671)\n",
      "(('to', 'be'), 0.001488013001738193)\n",
      "(('for', 'the'), 0.0014387392451859404)\n",
      "(('that', 'the'), 0.0013070703303474163)\n",
      "(('and', 'the'), 0.0012742211593125812)\n",
      "(('trump', 's'), 0.00120552827955189)\n",
      "(('at', 'the'), 0.001148269089156915)\n",
      "(('donald', 'trump'), 0.0011043492030219423)\n",
      "(('with', 'the'), 0.0010651479823394871)\n",
      "(('in', 'a'), 0.0010280338636841071)\n",
      "(('is', 'a'), 0.0009880159509041008)\n",
      "(('from', 'the'), 0.0009253121465254516)\n",
      "(('by', 'the'), 0.0009186878661786477)\n",
      "(('don', 't'), 0.0008626990857131968)\n",
      "(('of', 'a'), 0.0008436429367703367)\n",
      "(('that', 'he'), 0.00083021288894394)\n",
      "(('as', 'a'), 0.0008065288181149567)\n",
      "(('he', 's'), 0.0007372007333895036)\n",
      "(('has', 'been'), 0.000700631076132491)\n",
      "(('this', 'is'), 0.00067222833985099)\n",
      "(('that', 's'), 0.0006640614188754785)\n",
      "(('one', 'of'), 0.0006411940401440464)\n",
      "(('he', 'was'), 0.0006370198360898961)\n",
      "(('it', 'was'), 0.0006314844785398271)\n",
      "(('is', 'the'), 0.0006267658130873093)\n",
      "(('the', 'united'), 0.0006241342496618667)\n",
      "(('going', 'to'), 0.0006199600456077165)\n",
      "(('have', 'been'), 0.000618145174279825)\n",
      "(('it', 'is'), 0.0006178729435806414)\n",
      "(('hillary', 'clinton'), 0.0006058040492501632)\n",
      "(('about', 'the'), 0.0005862034389089357)\n",
      "(('white', 'house'), 0.0005723196732505661)\n",
      "(('image', 'via'), 0.00056406200870866)\n",
      "(('according', 'to'), 0.0005630638294783197)\n",
      "(('united', 'states'), 0.0005618841631151903)\n",
      "(('to', 'a'), 0.0005502689866166851)\n",
      "(('with', 'a'), 0.0005385630665517853)\n",
      "(('for', 'a'), 0.000509797356004706)\n",
      "(('as', 'the'), 0.000503626793489875)\n",
      "(('out', 'of'), 0.0004978192052406225)\n",
      "(('didn', 't'), 0.0004941894625848396)\n",
      "(('of', 'his'), 0.0004797612355281026)\n",
      "(('to', 'do'), 0.0004787630562977623)\n",
      "(('doesn', 't'), 0.00046787382833041364)\n",
      "(('s', 'a'), 0.00046097731728442616)\n",
      "(('will', 'be'), 0.00046097731728442616)\n",
      "Top 50 Bigrams after removing punctuations and stop words\n",
      "(('donald', 'trump'), 0.0011043492030219423)\n",
      "(('hillary', 'clinton'), 0.0006058040492501632)\n",
      "(('white', 'house'), 0.0005723196732505661)\n",
      "(('image', 'via'), 0.00056406200870866)\n",
      "(('united', 'states'), 0.0005618841631151903)\n",
      "(('new', 'york'), 0.00038076000459162445)\n",
      "(('president', 'obama'), 0.00033983465614767245)\n",
      "(('president', 'trump'), 0.0003239545320286223)\n",
      "(('fox', 'news'), 0.00026542493170412336)\n",
      "(('barack', 'obama'), 0.00020072476886479344)\n",
      "(('donald', 'j.'), 0.00019346528355322767)\n",
      "(('j.', 'trump'), 0.00019074297656139052)\n",
      "(('featured', 'image'), 0.0001894725666318665)\n",
      "(('century', 'wire'), 0.00017477210887594584)\n",
      "(('fake', 'news'), 0.00016043462538560345)\n",
      "(('supreme', 'court'), 0.00016016239468641974)\n",
      "(('national', 'security'), 0.00014981762811743852)\n",
      "(('21st', 'century'), 0.00014927316671907106)\n",
      "(('social', 'media'), 0.0001489101924534928)\n",
      "(('obama', 'administration'), 0.0001459156547624719)\n",
      "(('law', 'enforcement'), 0.00014183219427471615)\n",
      "(('york', 'times'), 0.00013693204168940929)\n",
      "(('state', 'department'), 0.0001345727089631504)\n",
      "(('washington', 'post'), 0.00013402824756478298)\n",
      "(('republican', 'party'), 0.00013076147917457837)\n",
      "(('american', 'people'), 0.00012994478707702723)\n",
      "(('attorney', 'general'), 0.00012767619791716293)\n",
      "(('lives', 'matter'), 0.0001237742245621963)\n",
      "(('bernie', 'sanders'), 0.00012277604533185603)\n",
      "(('black', 'lives'), 0.00012105191757035916)\n",
      "(('ted', 'cruz'), 0.00011751291848097084)\n",
      "(('bill', 'clinton'), 0.00011660548281702513)\n",
      "(('mainstream', 'media'), 0.00011460912435634454)\n",
      "(('last', 'year'), 0.00011243127876287481)\n",
      "(('even', 'though'), 0.0001111608688333508)\n",
      "(('last', 'week'), 0.00011025343316940508)\n",
      "(('trump', 'supporters'), 9.428256548396039e-05)\n",
      "(('trump', 'said'), 9.128802779293951e-05)\n",
      "(('presidential', 'candidate'), 9.10157970937558e-05)\n",
      "(('climate', 'change'), 9.056207926178293e-05)\n",
      "(('police', 'officers'), 8.820274653552407e-05)\n",
      "(('trump', 'administration'), 8.820274653552407e-05)\n",
      "(('president', 'donald'), 8.747679800436749e-05)\n",
      "(('trump', 'campaign'), 8.312110681742802e-05)\n",
      "(('years', 'ago'), 8.239515828627145e-05)\n",
      "(('health', 'care'), 8.103400479035287e-05)\n",
      "(('democratic', 'party'), 8.076177409116916e-05)\n",
      "(('make', 'sure'), 8.058028695838001e-05)\n",
      "(('first', 'time'), 8.039879982559086e-05)\n",
      "(('president', 'barack'), 7.976359486082886e-05)\n"
     ]
    }
   ],
   "source": [
    "print(\"Top 50 Bigrams\")\n",
    "for i in range(50):\n",
    "    print (scored1[i])\n",
    "\n",
    "print(\"Top 50 Bigrams after removing punctuations\")\n",
    "for i in range(50):\n",
    "    print (scored2[i])\n",
    "    \n",
    "print(\"Top 50 Bigrams after removing punctuations and stop words\")\n",
    "for i in range(50):\n",
    "    print (scored3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aec22623-28d2-444d-85bb-0c0d61bff6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('//t.co/ltdtbehhgh', 'pic.twitter.com/t2s8ufif5o'), 21.071701302752402)\n",
      "(('0000', '0907'), 21.071701302752402)\n",
      "(('0907', '84b4'), 21.071701302752402)\n",
      "(('314a', '3453'), 21.071701302752402)\n",
      "(('3453', '0000'), 21.071701302752402)\n",
      "(('4d6c', '6330'), 21.071701302752402)\n",
      "(('5707', '5736'), 21.071701302752402)\n",
      "(('5774', '6a7a'), 21.071701302752402)\n",
      "(('6330', '666b'), 21.071701302752402)\n",
      "(('666b', '314a'), 21.071701302752402)\n",
      "(('6a7a', '4d6c'), 21.071701302752402)\n",
      "(('7616', '86f7'), 21.071701302752402)\n",
      "(('84b4', 'f787'), 21.071701302752402)\n",
      "(('86f7', 'a737'), 21.071701302752402)\n",
      "(('a737', '5707'), 21.071701302752402)\n",
      "(('acab', 'pic.twitter.com/naqnehnd5g'), 21.071701302752402)\n",
      "(('f787', '7616'), 21.071701302752402)\n",
      "(('kambree', 'kawahine'), 21.071701302752402)\n",
      "(('kawahine', 'koa'), 21.071701302752402)\n",
      "(('lynnette', 'hardway'), 21.071701302752402)\n",
      "(('managementvideo', 'solutionsvideo'), 21.071701302752402)\n",
      "(('myocardial', 'infarction'), 21.071701302752402)\n",
      "(('palos', 'verdes'), 21.071701302752402)\n",
      "(('pic.twitter.com/pxbrcgypwm', \"'gitmo\"), 21.071701302752402)\n",
      "(('platformvideo', 'managementvideo'), 21.071701302752402)\n",
      "(('r.t.', 'rybak'), 21.071701302752402)\n",
      "(('today.4767', '5774'), 21.071701302752402)\n",
      "(('vis-', '-vis'), 21.071701302752402)\n",
      "(('300m', 'employee-related'), 20.808666896918606)\n",
      "(('bacha', 'bazi'), 20.808666896918606)\n",
      "(('bucolic', 'adirondacks'), 20.808666896918606)\n",
      "(('fern', 'ndez'), 20.808666896918606)\n",
      "(('kel', 'inen'), 20.808666896918606)\n",
      "(('kevork', 'djansezian/getty'), 20.808666896918606)\n",
      "(('miyoshi', 'jager'), 20.808666896918606)\n",
      "(('mondaiale', 'commerciale'), 20.808666896918606)\n",
      "(('paolo', 'gentiloni'), 20.808666896918606)\n",
      "(('psi', 'upsilon'), 20.808666896918606)\n",
      "(('sedrick', 'tydus'), 20.808666896918606)\n",
      "(('semper', 'fi'), 20.808666896918606)\n",
      "(('283', '-138'), 20.586274475582158)\n",
      "(('bendavid', 'grabinski'), 20.586274475582158)\n",
      "(('dani', 'bostick'), 20.586274475582158)\n",
      "(('g.k.', 'butterfield'), 20.586274475582158)\n",
      "(('neilson', 'barnard/getty'), 20.586274475582158)\n",
      "(('pic.twitter.com/fs297jooqi', 'vivelafrance'), 20.586274475582158)\n",
      "(('ria', 'novosti'), 20.586274475582158)\n",
      "(('roller', 'coaster'), 20.586274475582158)\n",
      "(('svenska', 'dagbladet'), 20.586274475582158)\n",
      "(('yik', 'yak'), 20.586274475582158)\n"
     ]
    }
   ],
   "source": [
    "from nltk.collocations import BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder4 = BigramCollocationFinder.from_words(fakeWords)\n",
    "finder4.apply_freq_filter(5)\n",
    "fakepmi = finder4.score_ngrams(bigram_measures.pmi)\n",
    "\n",
    "for i in range(50):\n",
    "    print(fakepmi[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9561cc68-1299-4999-b4d7-611b632ab2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saisrivishwanath/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saisrivishwanath/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'lemmatize']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wnl = nltk.WordNetLemmatizer()\n",
    "type(wnl)\n",
    "dir(wnl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1d65b8b-445c-45cf-b16f-4bb5b36be4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakelemma =[wnl.lemmatize(t) for t in fakeTokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1735ff4f-2b29-4149-a54b-f80cc7da6949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round( 100 - (len(set(fakelemma))/len(set(fakeTokens)) * 100), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb277492-c8db-44fb-8c2c-f18e1cb18f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('the', 483869)\n",
      "(',', 483695)\n",
      "('.', 295574)\n",
      "('to', 287018)\n",
      "('a', 255484)\n",
      "('of', 233763)\n",
      "('and', 214709)\n",
      "('in', 154163)\n",
      "('that', 143517)\n",
      "('s', 128873)\n",
      "('is', 108577)\n",
      "('for', 89271)\n",
      "('on', 78295)\n",
      "('Trump', 73159)\n",
      "('it', 71693)\n",
      "('wa', 67248)\n",
      "('he', 62634)\n",
      "('with', 61547)\n",
      "(':', 60195)\n",
      "('his', 55957)\n",
      "('be', 48237)\n",
      "('by', 45982)\n",
      "('have', 45660)\n",
      "('are', 45483)\n",
      "('not', 44243)\n",
      "('this', 44183)\n",
      "('I', 43566)\n",
      "(')', 42754)\n",
      "('(', 42446)\n",
      "('ha', 42033)\n",
      "('who', 41243)\n",
      "('The', 41223)\n",
      "('t', 40161)\n",
      "('from', 39697)\n",
      "('at', 38555)\n",
      "('they', 37233)\n",
      "('an', 34012)\n",
      "('you', 33199)\n",
      "('about', 32425)\n",
      "('said', 31001)\n",
      "('their', 29604)\n",
      "('?', 28310)\n",
      "('@', 28177)\n",
      "('we', 26527)\n",
      "('her', 24931)\n",
      "('people', 24842)\n",
      "('or', 24043)\n",
      "('but', 23485)\n",
      "('will', 23398)\n",
      "('would', 23218)\n",
      "('been', 22914)\n",
      "('out', 22607)\n",
      "('all', 22595)\n",
      "('were', 21475)\n",
      "('one', 21007)\n",
      "('what', 20591)\n",
      "('she', 20494)\n",
      "('more', 20343)\n",
      "('had', 20265)\n",
      "('can', 20092)\n",
      "('just', 18632)\n",
      "('our', 18204)\n",
      "('Clinton', 17957)\n",
      "('up', 17946)\n",
      "('when', 17818)\n",
      "('Obama', 17716)\n",
      "('like', 17575)\n",
      "('which', 17219)\n",
      "('!', 16976)\n",
      "('him', 16691)\n",
      "('President', 16328)\n",
      "('Donald', 16134)\n",
      "('It', 16128)\n",
      "('if', 15928)\n",
      "('say', 15327)\n",
      "('no', 15321)\n",
      "('so', 15147)\n",
      "('He', 14831)\n",
      "('also', 14757)\n",
      "('there', 14628)\n",
      "('do', 14511)\n",
      "('time', 14492)\n",
      "('after', 14375)\n",
      "('year', 14228)\n",
      "('because', 14128)\n",
      "('them', 13776)\n",
      "('than', 13635)\n",
      "('over', 13530)\n",
      "('Hillary', 13464)\n",
      "('into', 13314)\n",
      "('other', 12822)\n",
      "('even', 12520)\n",
      "('only', 12265)\n",
      "('how', 12108)\n",
      "('being', 12081)\n",
      "('know', 11861)\n",
      "('get', 11780)\n",
      "('right', 11306)\n",
      "('We', 11259)\n",
      "('any', 11101)\n"
     ]
    }
   ],
   "source": [
    "fakelemmadist = FreqDist(fakelemma)\n",
    "topfakelemmadist = fakelemmadist.most_common(100)\n",
    "for w in topfakelemmadist:\n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c056db93-461b-4e90-b678-5449a0833a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.lang.en.English"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "type(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49bae416-25ff-42d9-a669-bc0f0b2441cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeladvs = []\n",
    "for i in range(len(fake)):\n",
    "    spfake = nlp(fake[i])\n",
    "    fakeladvs.append((j, j.pos_) for j in spfake if j.pos_ == 'ADJ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "adea917b-a8f1-415d-80a0-edc85e302e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((new, 'ADJ'), 1), ((new, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((military, 'ADJ'), 1), ((small, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((initial, 'ADJ'), 1), ((mechanical, 'ADJ'), 1), ((short, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((public, 'ADJ'), 1), ((top, 'ADJ'), 1), ((secret, 'ADJ'), 1), ((governmental, 'ADJ'), 1), ((military, 'ADJ'), 1), ((Israeli, 'ADJ'), 1), ((worse, 'ADJ'), 1), ((American, 'ADJ'), 1), ((historic, 'ADJ'), 1), ((nuclear, 'ADJ'), 1), ((latest, 'ADJ'), 1), ((naive, 'ADJ'), 1), ((bizarre, 'ADJ'), 1), ((controversial, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((likely, 'ADJ'), 1), ((quick, 'ADJ'), 1), ((pro, 'ADJ'), 1), ((-, 'ADJ'), 1), ((war, 'ADJ'), 1), ((aggressive, 'ADJ'), 1), ((military, 'ADJ'), 1), ((hostile, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((Iranian, 'ADJ'), 1), ((giant, 'ADJ'), 1), ((less, 'ADJ'), 1), ((imaginary, 'ADJ'), 1), ((responsible, 'ADJ'), 1), ((American, 'ADJ'), 1), ((nuclear, 'ADJ'), 1), ((military, 'ADJ'), 1), ((veiled, 'ADJ'), 1), ((nuclear, 'ADJ'), 1), ((military, 'ADJ'), 1), ((top, 'ADJ'), 1), ((international, 'ADJ'), 1), ((such, 'ADJ'), 1), ((full, 'ADJ'), 1)]\n"
     ]
    }
   ],
   "source": [
    "fakeadjdist = FreqDist(fakeladvs[i])\n",
    "top50fakeadjdist = fakeadjdist.most_common(50)\n",
    "print(top50fakeadjdist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0cf54cc7-c12e-41a3-847c-87c6680c526f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 59815411 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spfake \u001b[38;5;241m=\u001b[39m \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfakeText\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:1030\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1011\u001b[0m     text: Union[\u001b[38;5;28mstr\u001b[39m, Doc],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     component_cfg: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Dict[\u001b[38;5;28mstr\u001b[39m, Any]]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1015\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Apply the pipeline to some text. The text can span multiple sentences,\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m    and can contain arbitrary whitespace. Alignment into the original string\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m    is preserved.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[38;5;124;03m    DOCS: https://spacy.io/api/language#call\u001b[39;00m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1030\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m component_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1032\u001b[0m         component_cfg \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:1121\u001b[0m, in \u001b[0;36mLanguage._ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc_like\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_like\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(doc_like, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Doc(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab)\u001b[38;5;241m.\u001b[39mfrom_bytes(doc_like)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/spacy/language.py:1110\u001b[0m, in \u001b[0;36mLanguage.make_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Turn a text into a Doc object.\u001b[39;00m\n\u001b[1;32m   1105\u001b[0m \n\u001b[1;32m   1106\u001b[0m \u001b[38;5;124;03mtext (str): The text to process.\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;124;03mRETURNS (Doc): The processed doc.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length:\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1111\u001b[0m         Errors\u001b[38;5;241m.\u001b[39mE088\u001b[38;5;241m.\u001b[39mformat(length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(text), max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_length)\n\u001b[1;32m   1112\u001b[0m     )\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text)\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 59815411 exceeds maximum of 1000000. The parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "spfake = nlp(fakeText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c8009-0e10-4456-ad49-b01ded455ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
